# Google ADK AWS Integrations - Implementation Plan (External Repository)

## Overview

This document provides a comprehensive implementation plan for creating AWS service integrations for the ADK (Agent Development Kit) as a **separate external package**. Starting with S3 Artifact Service implementation, this package positions itself as a foundation for comprehensive AWS service integrations, following the same patterns as existing ADK services while adapting to AWS-specific APIs and best practices.

## External Repository Strategy

This implementation assumes development in a separate repository that will be:
- **Standalone Package**: Distributed as `google-adk-aws`
- **AWS Integrations Package**: Foundation for multiple AWS service integrations (S3, Lambda, SQS, etc.)
- **Extensible Design**: Structured to support future AWS services beyond S3
- **Community Contribution**: Potentially contributed back to the main ADK repo later

## 1. Architecture Analysis

### Current GCS Implementation Patterns
- **Path Structure**: `{app_name}/{user_id}/{session_id|user}/{filename}/{version}`
- **User Namespace**: Files prefixed with `user:` are stored in user-level scope
- **Version Management**: Sequential integer versions starting from 0
- **MIME Type Handling**: Automatic content type detection and storage
- **Async Interface**: All operations are async-compatible

### S3 Adaptation Strategy
- **Object Keys**: Use same hierarchical path structure as GCS
- **Metadata**: Store MIME type and version info in S3 object metadata
- **Listing**: Use S3 prefix-based listing for efficient queries
- **Versioning**: Implement application-level versioning (not S3 object versioning)

## 2. External Package Setup

### Repository Structure
```
google-adk-aws/
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ google_adk_aws/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ s3_artifact_service.py
‚îÇ       ‚îî‚îÄ‚îÄ py.typed
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_s3_artifact_service.py
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îÇ       ‚îî‚îÄ‚îÄ test_s3_integration.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ installation.md
‚îÇ   ‚îú‚îÄ‚îÄ configuration.md
‚îÇ   ‚îî‚îÄ‚îÄ examples.md
‚îî‚îÄ‚îÄ examples/
    ‚îú‚îÄ‚îÄ basic_usage.py
    ‚îî‚îÄ‚îÄ production_setup.py
```

### Package Dependencies
```toml
# pyproject.toml for external package
[project]
name = "google-adk-aws"
version = "0.1.0"
description = "AWS service integrations for Google Agent Development Kit (ADK)"
dependencies = [
    "google-adk>=1.2.0",               # Core ADK dependency
    "boto3>=1.34.0",                   # AWS SDK for Python
    "botocore>=1.34.0",                # Core functionality for boto3
    "google-genai>=1.17.0",            # For types.Part compatibility
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",
    "moto[s3]>=4.2.0",                 # S3 mocking for tests
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.5.0",
    "ruff>=0.1.0",                     # Fast Python linter
    "pylint>=3.0.0",                   # Additional code quality checks
]
test = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",
    "moto[s3]>=4.2.0"
]
```

### ADK Integration Strategy
The external package will be designed to integrate seamlessly:
```python
# Option 1: Import and register manually
from google_adk_aws import S3ArtifactService
artifact_service = S3ArtifactService(bucket_name="my-bucket")

# Option 2: Future ADK integration via extensions registry
# (If ADK adds plugin system)
```

### Modern Python Features & Tooling
- **Python 3.10+ Requirements**: Utilizes modern type hints and union syntax
- **Enhanced Development Tools**: Comprehensive tooling setup with ruff, black, isort, mypy
- **Comprehensive Testing**: Full test coverage with pytest, moto mocking, and coverage reporting
- **Development Workflow**: Makefile-driven development with CI/CD integration
- **Code Quality**: Automated formatting, linting, and type checking

### Configuration Requirements
- **AWS Credentials**: Support multiple authentication methods
  - Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
  - IAM roles (for EC2/ECS deployment)
  - AWS credentials file
  - STS assume role
- **Region Configuration**: Configurable AWS region
- **Bucket Configuration**: Configurable S3 bucket name
- **Optional Settings**: Custom endpoint URL (for S3-compatible services)

## 3. Implementation Plan

### Phase 1: External Package Setup

#### 1.1 Repository Initialization
```bash
# Create new repository
git init google-adk-aws
cd google-adk-aws

# Set up Python package structure
mkdir -p src/google_adk_aws tests/unit tests/integration docs examples
touch src/google_adk_aws/__init__.py
touch src/google_adk_aws/py.typed
```

#### 1.2 Package Configuration
**Location**: `pyproject.toml`
```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "google-adk-aws"
dynamic = ["version"]
description = "AWS service integrations for Google Agent Development Kit (ADK)"
readme = "README.md"
license = {file = "LICENSE"}
authors = [{name = "Joshua Shanks", email = "jjshanks@gmail.com"}]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
requires-python = ">=3.10+"
dependencies = [
    "google-adk>=1.2.0",
    "boto3>=1.34.0",
    "botocore>=1.34.0",
]

[project.urls]
Homepage = "https://github.com/jjshanks/google-adk-aws"
Repository = "https://github.com/jjshanks/google-adk-aws.git"
Issues = "https://github.com/jjshanks/google-adk-aws/issues"

[tool.hatch.version]
path = "src/google_adk_aws/__init__.py"

[tool.hatch.build.targets.wheel]
packages = ["src/google_adk_aws"]
```

#### 1.2.3 Development Infrastructure (Makefile)
**Location**: `Makefile`
```makefile
# Makefile for google-adk-aws development

.PHONY: help dev-install test test-unit test-integration test-coverage lint format typecheck build dist clean ci

# Default target
help:
	@echo "Available targets:"
	@echo "  dev-install    Install package in development mode with all dependencies"
	@echo "  test           Run all tests"
	@echo "  test-unit      Run unit tests only"
	@echo "  test-integration Run integration tests only"
	@echo "  test-coverage  Run tests with coverage report"
	@echo "  lint           Run code linting (ruff, pylint)"
	@echo "  format         Format code (black, isort)"
	@echo "  typecheck      Run type checking (mypy)"
	@echo "  build          Build package"
	@echo "  dist           Create distribution packages"
	@echo "  clean          Clean build artifacts"
	@echo "  ci             Run full CI pipeline (format, lint, typecheck, test)"

# Development setup
dev-install:
	pip install -e ".[dev,test]"

# Testing targets
test:
	pytest tests/

test-unit:
	pytest tests/unit/ -v

test-integration:
	pytest tests/integration/ -v

test-coverage:
	pytest --cov=google_adk_aws --cov-report=html --cov-report=term-missing

# Code quality
lint:
	ruff check src/ tests/
	pylint src/google_adk_aws/

format:
	black src/ tests/
	isort src/ tests/

format-check:
	black --check src/ tests/
	isort --check-only src/ tests/

typecheck:
	mypy src/google_adk_aws/

# Build and distribution
build:
	python -m build

dist: clean build
	ls -la dist/

clean:
	rm -rf build/
	rm -rf dist/
	rm -rf *.egg-info/
	rm -rf htmlcov/
	find . -type d -name __pycache__ -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete

# CI pipeline
ci: format-check lint typecheck test-coverage
	@echo "All CI checks passed!"

# Development workflow
dev-test: format lint typecheck test-unit
	@echo "Development checks completed!"
```

#### 1.3 Create `s3_artifact_service.py`
**Location**: `src/google_adk_aws/s3_artifact_service.py`

**Key Components**:
```python
"""S3 Artifact Service implementation for Google ADK."""

import asyncio
import logging
from typing import Optional

import boto3
import botocore.exceptions
from google.adk.artifacts import BaseArtifactService
from google.genai import types

__version__ = "0.1.0"

class S3ArtifactService(BaseArtifactService):
    def __init__(
        self,
        bucket_name: str,
        region_name: str = 'us-east-1',
        aws_access_key_id: Optional[str] = None,
        aws_secret_access_key: Optional[str] = None,
        aws_session_token: Optional[str] = None,
        endpoint_url: Optional[str] = None,
        **kwargs
    ):
        # Initialize boto3 S3 client with authentication

    def _get_object_key(self, app_name, user_id, session_id, filename, version) -> str:
        # Same logic as GCS _get_blob_name but return S3 object key

    def _file_has_user_namespace(self, filename: str) -> bool:
        # Identical to GCS implementation

    async def save_artifact(...) -> int:
        # 1. Get existing versions
        # 2. Calculate next version number
        # 3. Upload to S3 with metadata
        # 4. Return version number

    async def load_artifact(...) -> Optional[types.Part]:
        # 1. Determine version (latest if None)
        # 2. Download from S3
        # 3. Create types.Part from data

    async def list_artifact_keys(...) -> list[str]:
        # 1. List objects with session prefix
        # 2. List objects with user namespace prefix
        # 3. Extract filenames and return sorted list

    async def delete_artifact(...) -> None:
        # 1. List all versions
        # 2. Delete each version

    async def list_versions(...) -> list[int]:
        # 1. List objects with filename prefix
        # 2. Extract version numbers
        # 3. Return sorted list
```

#### 1.4 Package `__init__.py`
**Location**: `src/google_adk_aws/__init__.py`

```python
"""AWS service integrations for Google Agent Development Kit (ADK)."""

from .s3_artifact_service import S3ArtifactService

__version__ = "0.1.0"
__all__ = ["S3ArtifactService"]
```

#### 1.5 Create README and Documentation
**Location**: `README.md`

```markdown
# Google ADK AWS Integrations

AWS service integrations for Google Agent Development Kit (ADK), starting with S3 Artifact Service.

## Installation

```bash
pip install google-adk-aws
```

## Quick Start

```python
from google_adk_aws import S3ArtifactService
from google.adk.agents import Agent

# Create S3 artifact service
artifact_service = S3ArtifactService(
    bucket_name="my-artifacts-bucket",
    region_name="us-west-2"
)

# Use with ADK agent
agent = Agent(
    name="my_agent",
    model="gemini-2.0-flash",
    artifact_service=artifact_service
)
```

## Features

### Current (Phase 1)
- **S3 Artifact Service**: Complete BaseArtifactService implementation for S3 storage
- **Flexible Authentication**: Support for IAM roles, access keys, and custom endpoints
- **Version Management**: Automatic artifact versioning with history tracking
- **User Namespace Support**: Session-scoped and user-scoped artifact storage
- **Modern Python**: Built with Python 3.10+ features and comprehensive tooling

### Future Roadmap
- **Lambda Integration**: Function execution and management tools
- **SQS Integration**: Message queue operations and workflow triggers
- **DynamoDB Tools**: NoSQL database integrations for agent state
- **CloudWatch Integration**: Monitoring and logging capabilities
- **EventBridge Tools**: Event-driven architecture support
- **Secrets Manager**: Secure credential and configuration management

## Documentation

- [Installation Guide](docs/installation.md)
- [Configuration Options](docs/configuration.md)
- [Usage Examples](docs/examples.md)
```

### Phase 2: Detailed Implementation

#### 2.1 Authentication Implementation
```python
def _create_s3_client(self):
    """Create S3 client with proper authentication."""
    session = boto3.Session(
        aws_access_key_id=self.aws_access_key_id,
        aws_secret_access_key=self.aws_secret_access_key,
        aws_session_token=self.aws_session_token,
        region_name=self.region_name
    )

    client_config = {}
    if self.endpoint_url:
        client_config['endpoint_url'] = self.endpoint_url

    return session.client('s3', **client_config)
```

#### 2.2 Save Artifact Implementation
```python
async def save_artifact(self, *, app_name: str, user_id: str, session_id: str,
                       filename: str, artifact: types.Part) -> int:
    # Get existing versions to determine next version number
    versions = await self.list_versions(
        app_name=app_name, user_id=user_id,
        session_id=session_id, filename=filename
    )
    version = 0 if not versions else max(versions) + 1

    # Construct S3 object key
    object_key = self._get_object_key(
        app_name, user_id, session_id, filename, version
    )

    # Prepare metadata
    metadata = {
        'Content-Type': artifact.inline_data.mime_type,
        'app-name': app_name,
        'user-id': user_id,
        'session-id': session_id,
        'filename': filename,
        'version': str(version)
    }

    # Upload to S3
    await asyncio.get_event_loop().run_in_executor(
        None,
        lambda: self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=object_key,
            Body=artifact.inline_data.data,
            ContentType=artifact.inline_data.mime_type,
            Metadata=metadata
        )
    )

    return version
```

#### 2.3 Load Artifact Implementation
```python
async def load_artifact(self, *, app_name: str, user_id: str, session_id: str,
                       filename: str, version: Optional[int] = None) -> Optional[types.Part]:
    # Determine version if not specified
    if version is None:
        versions = await self.list_versions(
            app_name=app_name, user_id=user_id,
            session_id=session_id, filename=filename
        )
        if not versions:
            return None
        version = max(versions)

    # Construct object key
    object_key = self._get_object_key(
        app_name, user_id, session_id, filename, version
    )

    try:
        # Download from S3
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: self.s3_client.get_object(
                Bucket=self.bucket_name,
                Key=object_key
            )
        )

        # Read data
        data = response['Body'].read()
        content_type = response['ContentType']

        # Create types.Part
        return types.Part.from_bytes(data=data, mime_type=content_type)

    except self.s3_client.exceptions.NoSuchKey:
        return None
```

#### 2.4 List Operations Implementation
```python
async def list_artifact_keys(self, *, app_name: str, user_id: str, session_id: str) -> list[str]:
    filenames = set()

    # List session-scoped artifacts
    session_prefix = f"{app_name}/{user_id}/{session_id}/"
    await self._list_with_prefix(session_prefix, filenames)

    # List user-scoped artifacts
    user_prefix = f"{app_name}/{user_id}/user/"
    await self._list_with_prefix(user_prefix, filenames)

    return sorted(list(filenames))

async def _list_with_prefix(self, prefix: str, filenames: set):
    """Helper method to list objects with a given prefix."""
    paginator = self.s3_client.get_paginator('list_objects_v2')

    async for page in asyncio.get_event_loop().run_in_executor(
        None,
        lambda: paginator.paginate(
            Bucket=self.bucket_name,
            Prefix=prefix
        )
    ):
        for obj in page.get('Contents', []):
            # Extract filename from key: app/user/session/filename/version
            key_parts = obj['Key'].split('/')
            if len(key_parts) >= 5:
                filename = key_parts[-2]  # Second to last part is filename
                filenames.add(filename)
```

### Phase 3: Error Handling and Edge Cases

#### 3.1 Error Handling Strategy
```python
# Custom exceptions for S3-specific errors
class S3ArtifactError(Exception):
    """Base exception for S3 artifact operations."""
    pass

class S3ConnectionError(S3ArtifactError):
    """Raised when S3 connection fails."""
    pass

class S3PermissionError(S3ArtifactError):
    """Raised when S3 permissions are insufficient."""
    pass

# Wrap S3 operations with proper error handling
try:
    response = self.s3_client.put_object(...)
except botocore.exceptions.ClientError as e:
    error_code = e.response['Error']['Code']
    if error_code == 'AccessDenied':
        raise S3PermissionError(f"Access denied to bucket {self.bucket_name}")
    elif error_code == 'NoSuchBucket':
        raise S3ArtifactError(f"Bucket {self.bucket_name} does not exist")
    else:
        raise S3ArtifactError(f"S3 operation failed: {e}")
except botocore.exceptions.NoCredentialsError:
    raise S3ConnectionError("AWS credentials not found")
```

#### 3.2 Edge Cases to Handle
- **Large Files**: Implement multipart upload for files > 5GB
- **Concurrent Writes**: Handle race conditions in version assignment
- **Network Failures**: Implement retry logic with exponential backoff
- **Invalid Characters**: Sanitize object keys for S3 compatibility
- **Empty Artifacts**: Handle zero-byte files correctly

### Phase 4: Testing Strategy

#### 4.1 Unit Tests
**Location**: `tests/unit/test_s3_artifact_service.py`

**Test Categories**:
```python
class TestS3ArtifactService:
    def test_initialization():
        # Test various authentication configurations

    def test_save_artifact():
        # Test basic save operation
        # Test version increment
        # Test user namespace handling

    def test_load_artifact():
        # Test load latest version
        # Test load specific version
        # Test missing artifact returns None

    def test_list_operations():
        # Test list_artifact_keys
        # Test list_versions
        # Test empty results

    def test_delete_artifact():
        # Test delete all versions
        # Test delete non-existent artifact

    def test_error_handling():
        # Test invalid credentials
        # Test missing bucket
        # Test network errors

    def test_edge_cases():
        # Test special characters in filenames
        # Test large files
        # Test concurrent operations
```

#### 4.2 Integration Tests
**Location**: `tests/integration/test_s3_integration.py`

**Requirements**:
- Mock S3 service using moto library
- Test with real S3 bucket (optional, for CI/CD)
- Test compatibility with existing artifact tools

#### 4.3 Mocking Strategy
```python
# Use moto for S3 mocking
import moto
from moto import mock_s3

@mock_s3
def test_s3_operations():
    # Create mock S3 environment
    s3_client = boto3.client('s3', region_name='us-east-1')
    s3_client.create_bucket(Bucket='test-bucket')

    # Test S3ArtifactService operations
    service = S3ArtifactService(bucket_name='test-bucket')
    # ... test operations
```

### Phase 5: Documentation and Examples

#### 5.1 External Package Documentation
Create comprehensive documentation in the `docs/` directory:

**docs/installation.md**:
```markdown
# Installation Guide

## Prerequisites
- Python 3.9+
- Google ADK installed
- AWS account with S3 access

## Installation Options

### From PyPI (Recommended)
```bash
pip install google-adk-aws
```

### From Source
```bash
git clone https://github.com/yourusername/adk-s3-artifacts
cd adk-s3-artifacts
pip install -e .
```

## AWS Setup
1. Create S3 bucket
2. Configure IAM permissions
3. Set up AWS credentials
```

**docs/configuration.md**:
```markdown
# Configuration Guide

## Authentication Methods
- Environment variables
- IAM roles (recommended for production)
- AWS credentials file
- STS assume role

## Performance Tuning
- Connection pooling
- Multipart uploads
- Regional considerations
```

#### 5.2 Usage Examples
Create examples in the `examples/` directory:

**examples/basic_usage.py**:
```python
#!/usr/bin/env python3
"""Basic usage example for S3 Artifact Service."""

from google_adk_aws import S3ArtifactService
from google.adk.agents import Agent

def main():
    # Create S3 artifact service
    artifact_service = S3ArtifactService(
        bucket_name="my-artifacts-bucket",
        region_name="us-west-2"
    )

    # Use with ADK agent
    agent = Agent(
        name="document_processor",
        model="gemini-2.0-flash",
        artifact_service=artifact_service,
        instruction="Process documents and save results to S3"
    )

    # Run agent
    response = agent.run("Analyze this quarterly report")
    print(response)

if __name__ == "__main__":
    main()
```

**examples/production_setup.py**:
```python
#!/usr/bin/env python3
"""Production setup example with IAM role authentication."""

import os
from google_adk_aws import S3ArtifactService

def create_production_service():
    """Create S3 service for production deployment."""
    return S3ArtifactService(
        bucket_name=os.environ["S3_ARTIFACTS_BUCKET"],
        region_name=os.environ.get("AWS_REGION", "us-east-1"),
        # IAM role authentication (no keys needed)
    )
```

### Phase 6: Deployment Considerations

#### 6.1 Production Setup
- **IAM Roles**: Recommend IAM roles over access keys
- **Bucket Policies**: Provide sample bucket policy templates
- **VPC Endpoints**: Support for S3 VPC endpoints
- **Encryption**: Support for S3 server-side encryption

#### 6.2 Performance Optimization
- **Connection Pooling**: Reuse S3 client connections
- **Batch Operations**: Implement batch delete for multiple versions
- **Parallel Uploads**: Support concurrent artifact saves
- **Caching**: Consider implementing metadata caching

### Phase 7: Distribution and Community

#### 7.1 Package Publishing
```bash
# Build package
python -m build

# Test upload to TestPyPI
python -m twine upload --repository testpypi dist/*

# Production upload to PyPI
python -m twine upload dist/*
```

#### 7.2 CI/CD Setup
Create `.github/workflows/` for automated testing and publishing:

**test.yml**:
```yaml
name: Test
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, "3.10", "3.11", "3.12"]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          pip install -e .[dev]
      - name: Run tests
        run: |
          pytest tests/
```

#### 7.3 Community Contribution Strategy
- **Open Source**: Apache 2.0 license for compatibility with ADK
- **Documentation**: Comprehensive docs and examples
- **Issue Tracking**: GitHub issues for bug reports and feature requests
- **Contributing Guide**: Clear guidelines for community contributions
- **ADK Integration**: Propose adding to ADK's optional extensions

## Implementation Timeline

| Phase | Duration | Deliverables |
|-------|----------|--------------|
| 1 | 2-3 days | External package setup, core S3ArtifactService class, Makefile infrastructure |
| 2 | 3-4 days | Complete implementation with error handling |
| 3 | 2-3 days | Comprehensive error handling and edge cases |
| 4 | 3-4 days | Full test suite with mocking and CI/CD |
| 5 | 2-3 days | Documentation, examples, and README |
| 6 | 2-3 days | Production considerations and optimization |
| 7 | 1-2 days | Package publishing and community setup |

**Total Estimated Time**: 15-22 days

## Success Criteria

1. **Functional Parity**: S3ArtifactService implements all methods from BaseArtifactService
2. **Test Coverage**: >95% code coverage with comprehensive test suite
3. **Performance**: Comparable performance to GcsArtifactService for similar operations
4. **Documentation**: Complete documentation with usage examples
5. **Production Ready**: Proper error handling, logging, and security considerations
6. **Package Distribution**: Successfully published to PyPI and installable
7. **Community Ready**: Open source with contribution guidelines and CI/CD
8. **Modern Tooling**: Comprehensive development infrastructure with Makefile, ruff, black, mypy

## Risks and Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| AWS API changes | Medium | Pin boto3 versions, implement adapter pattern |
| Performance issues | High | Implement connection pooling and caching |
| Authentication complexity | Medium | Support multiple auth methods, clear documentation |
| ADK API changes | High | Pin ADK version, monitor for breaking changes |
| Package distribution issues | Medium | Test with TestPyPI first, automate publishing |
| Community adoption | Low | Comprehensive docs, examples, propose ADK integration |

## Implementation Status Update

**Phase 1 Complete** - The following enhancements were made during implementation:

### üîÑ Project Positioning Changes
- **Package Name**: Changed from `adk-s3-artifacts` to `google-adk-aws` for broader AWS integrations scope
- **Scope Expansion**: Positioned as comprehensive AWS integrations package (not just S3)
- **Future-Ready**: Structured to support Lambda, SQS, DynamoDB, and other AWS services

### üêç Modern Python Upgrade
- **Python Version**: Upgraded minimum requirement from 3.9+ to 3.10+
- **Type Hints**: Modernized to use Python 3.10+ union syntax (`str | None` vs `Optional[str]`)
- **Build System**: Updated from setuptools to hatchling for modern packaging

### üõ†Ô∏è Enhanced Development Infrastructure
- **Comprehensive Makefile**: 30+ targets for complete development workflow
- **Advanced Tooling**: Added ruff (fast linting), enhanced mypy configuration
- **Testing Framework**: Comprehensive pytest setup with moto S3 mocking
- **Development Dependencies**: Full dev/test dependency groups with version pinning

### üë§ Author & Repository Updates
- **Author**: Joshua Shanks (jjshanks@gmail.com)
- **Repository**: https://github.com/jjshanks/google-adk-aws
- **License**: MIT License (2025)

### üì¶ Package Configuration Enhancements
- **Modern Build**: Hatchling build system with proper wheel configuration
- **Comprehensive Dependencies**: Organized dev, test, and runtime dependency groups
- **Code Quality**: Integrated black, isort, ruff, mypy configurations
- **Coverage**: Proper pytest and coverage reporting setup

**Key Insight**: All core S3ArtifactService implementation remains exactly as specified - all changes were infrastructure, tooling, and positioning improvements that enhance the original plan without changing its technical requirements.
